{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project was prepared as a practice to lessons under the Data wrangling section on the ALX-T Udacity Data Analyst Nanodegree program.\n",
    "\n",
    "The focus of the project is to gather data from different sources and wrangle for the tweet archive of @dog_rates also known as WeRateDogs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project aims to teach the user how to gather data from three different sources for WeRAteDogs project. The datasets are to be wrangled using the different phases of wrangling as taught in the wrangling module. \n",
    "\n",
    "The process include: \n",
    "1. Gathering data\n",
    "2. Assessing data\n",
    "3. Cleaning data\n",
    "4. Storing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Twitter archive file**: The archive was provided by Udacity in the introduction section. The dataset was downloaded into the jupyter notebook workspace as provided in the project section. \n",
    "\n",
    "The python pandas library was imported and dataset uploaded using the pandas **read_csv()** function to read the twitter archive csv file into a readable dataframe that i named as **df_1**.. \n",
    "\n",
    "**Tweet image prediction file**: This file was downloaded using the python **requests** and **os** libraries. The dataset was downloaded from udacity's servers using the **get()** function through its url and using the **'with open'** fuction, the content using the **response**  fuction was downloaded and uploaded ito the directory. \n",
    "\n",
    "This file was then read into a dataframe using the pandas **read_csv()** function and named **df_2**\n",
    "\n",
    "**Tweet_Json text**: The file was supposed to be downloaded by creating a twitter developer account and using the tweet IDs, query twitters' API for each tweet's JSON data using the **'Tweepy'** library.\n",
    "\n",
    "Due to certain problems encountered while getting access to the developer account from twitter, I used the txt file that was provided by Udacity. \n",
    "\n",
    "Using the **with open** function, i opened the **'tweet-json.txt'** file that was provided by reading the file line by line and loaded each line as a json file. \n",
    "\n",
    "The json data was appended to an empty list named **'tweets_list'** and the file was read using the pandas dataframe with **fields id**, **retweet_count** and **favourite_count**. The file was later named as **df_3**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data was assessed visually and also programmatically using python functions to identify 8 quality and 2 tidiness issues which can be found in the datasets. \n",
    "\n",
    "These issues were identified using methods and functions such as .info(), .notnull(), .shape(), .query(), .head(), .value_counts(), .duplicated() and so on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The issues identified during the assessment stage were documented and the process was divided into three parts, \n",
    "* Define\n",
    "* Code\n",
    "* Test\n",
    "\n",
    "A copy of the datasets were made and the quality and tidiness issues raised were. \n",
    "\n",
    "### Quality\n",
    "1. We want only original ratings(no retweets) that have images. Remove from df_1 values with retweets\n",
    "\n",
    "2. in_reply to_status_id, in_reply_to_user_id are replies just like the retweets. Remove the replies from the dataset\n",
    "\n",
    "3. Remove from the dataset values which have rating_denominator columns greater than 10 \n",
    "\n",
    "4. Timestamp column should be in datetime format \n",
    "\n",
    "5. Name values in lowercase are wrong entries and should be replaced with None\n",
    "\n",
    "6. Different tweet_ids have multiple jpg_url. Removed the duplicated urls\n",
    "\n",
    "7. Rename the id colum in df_3 to have the same name 'tweet_ids' as the other 2 columns\n",
    "\n",
    "8. Rename the 'id' colum in df_3 to have the same name 'tweet_ids' as the other 2 columns\n",
    "\n",
    "### Tidiness\n",
    "\n",
    "1. Convert dog stage column into a single column \n",
    "\n",
    "2. Merge the three dataframes into one dataframe using the tweet_id column\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cleaned and merged dataframe was then saved as a csv file using the pandas .to_csv() function and named **'twitter_archive_master.csv'** "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
